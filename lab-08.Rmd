---
title: "Lab 08 - University of Edinburgh Art Collection"
author: "Insert your name here"
date: "Insert date here"
output: github_document
---

### Load packages and data



```{r load-packages, message = FALSE}
library(rvest)
library(skimr)
library(glue)
library(tidyverse) 
library(usethis)
```


### Exercise 1

```{r start}
# setting the url
first_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0"

# reading the html page
page <- read_html(first_url)

# Scraping titles

titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>% 
  str_squish()

# Scraping links

links <- page %>%
  html_nodes(".iteminfo") %>%   # same nodes
  html_node("h3 a") %>%         # as before
  html_attr("href") %>%         # but get href attribute instead of text
  str_replace(pattern =".", replacement = "https://collections.ed.ac.uk") #replacing link

```


### Exercise 2
```{R 2}
# scraping artists 

artists <- page %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_attr("title")
```


### Exercise 3

```{R 3}
first_ten <- tibble(
  title = titles,
  artist = artists,
  link = links)

first_ten
```
### Exercise 4

```{R scrape_2}
second_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=10"

page2 <- read_html(second_url)


titles2 <- page2 %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()

links2 <- page2 %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_attr("href") %>%
  str_replace(".", "https://collections.ed.ac.uk/art/")

artists2 <- page2 %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_text() %>%
  str_squish()

second_ten <- tibble(
  title = titles2,
  artist = artists2,
  link = links2)

second_ten
```

### Exercise 5

```{r scrape_funtion}
# test

add_two <- function(x){
  x + 2
}

add_two(3)

add_two(10)

# function: scrape_page --------------------------------------------------------

scrape_page <- function(url){
  # read page
  page <- read_html(url)
  
  # scrape titles
  titles <- page %>%
    html_nodes(".iteminfo") %>%
    html_node("h3 a") %>%
    html_text() %>% 
    str_squish()
  
  # scrape links
  links <- page %>%
    html_nodes(".iteminfo") %>%   # same nodes
    html_node("h3 a") %>%         # as before
    html_attr("href") %>%         # but get href attribute instead of text
    str_replace(pattern =".", replacement = "https://collections.ed.ac.uk") #replacing
  
  # scrape artists 
  names <- page %>% 
    html_nodes(".iteminfo") %>% 
    html_node(".artist") %>% 
    html_attr("title")
  
  # create and return tibble
  tibble(title = titles, 
         name = names,
         link = links)
  
}
```

### Exercise 6

```{r testing}

scrape_page(first_url)
scrape_page(second_url)

```

### Exercise 7

### Exercise 8

### Exercise 9

```{r separate-title-date, error = TRUE}
uoe_art <- uoe_art %>%
  separate(title, into = c("title", "date"), sep = "\\(") %>%
  mutate(year = str_remove(date, "\\)") %>% as.numeric()) %>%
  select(title, artist, year, ___)
```

### Exercise 10

Remove this text, and add your answer for Exercise 10 here.
Add code chunks as needed.
Don't forget to label your code chunk.
Do not use spaces in code chunk labels.

### Exercise 11

### Exercise 12

### Exercise 13

### Exercise 14

### Exercise 15

...

{r load-data, message = FALSE, eval = FALSE}
# Remove eval = FALSE or set it to TRUE once data is ready to be loaded
uoe_art <- read_csv("data/uoe-art.csv")
Add exercise headings as needed.
